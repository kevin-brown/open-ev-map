name: Daily Scrapers

on:
  schedule:
    - cron: '0 6,18 * * *'  # Runs every day at 6:00 and 18:00
  workflow_dispatch:  # Allows manual triggering of the workflow

env:
  SCRAPED_DATA_DIR: '${{ github.workspace }}/scraped_data'
  NREL_API_KEY: ${{ secrets.NREL_API_KEY }}

jobs:
  scrape:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scraper:
          - blink
          - chargepoint
          - electricera
          - electrifyamerica
          - enelx
          - energy5
          - evconnect
          - evconnect_chargesmartev
          - evgateway
          - evgateway_chargesmartev
          - evgateway_siements
          - evgo
          - evloop
          - evpassport
          - greenspot
          - flo
          - nrel_afdc
          - openstreetmap
          - powerchargeev
          - rivian
          - shellrecharge
          - skycharger
          - supercharge
          - zeplug
    steps:
      - name: Checkout scraper repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: pipenv

      - name: Set up pipenv
        run: |
          pip install --upgrade pipenv wheel

      - name: Install dependencies
        run: |
          pipenv install --dev

      - name: Run scraper
        run: |
          pipenv run scrapy crawl ${{ matrix.scraper }}

      - name: Upload scraped data
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.scraper }}-data
          path: ${{ env.SCRAPED_DATA_DIR }}/${{ matrix.scraper }}.json
          if-no-files-found: error
